{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da468dc-c543-4dd5-9bd5-d1ad2a780398",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Download and process the Wikipedia CirrusSearch index\n",
    "\n",
    "This is the actual search index dump from Wikipedia which backs the Dynamic Wikipedia feature.\n",
    "Here we download the index locally and stream through it to extract subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0961f4c-3f97-417c-a4cc-1730d29a550e",
   "metadata": {},
   "source": [
    "## Warning! Do not run this full notebook unless really necessary\n",
    "\n",
    "__This will download the full ES index dump and save it to disk.\n",
    "The file is ~35 GB. Make sure you have enough free disk space.\n",
    "It may take several hours to run.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1952ebdf-a53e-4e1d-ac1b-d245bd16ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from wikipedia_utils import search_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5786bcc-6c6a-426f-a347-6b55014cc329",
   "metadata": {},
   "source": [
    "Data files will be written to the following subdir using predefined filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63553e1-9d4f-4305-92e2-af786903e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_INDEX_DIR = Path(\"es_data\")\n",
    "SEARCH_INDEX_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f341efe5-0e6b-49eb-aeb5-78845b7f49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCKED_CATS_CSV = Path(\"category_data\") / \"blocklist_cats.csv\"\n",
    "BLOCKED_PAGES_INDEX = SEARCH_INDEX_DIR / \"blocked_cirrussearch.json.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb82bbb6-3c26-4655-a21b-becd745a468d",
   "metadata": {},
   "source": [
    "## Download the search index dump\n",
    "\n",
    "The full index file is gzipped 35 GB.\n",
    "In the ElasticSearch index format, entries are composed of two lines\n",
    "```\n",
    "{\"index\": {...}}\n",
    "{field1: val1, ...}\n",
    "```\n",
    "The page information we are interested in is on the second line of each entry.\n",
    "\n",
    "Download the latest verison of the file. It is usually around 35 GB(!). The download may take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "626adb91-acbd-4e0f-83f8-76ae217a4081",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_dump_date, latest_dump_size = search_index.get_latest_search_dump_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8908c3a5-e274-4085-95e8-6862753e0d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest dump date: 20230403\n",
      "Index file size: 34.45 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Latest dump date: {latest_dump_date}\")\n",
    "print(f\"Index file size: {latest_dump_size / (1024*1024*1024):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "41dc54fb-7d40-43d9-821e-1fdcb4054461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 57s, sys: 3min 3s, total: 5min 1s\n",
      "Wall time: 2h 31min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "search_index.fetch_search_index(latest_dump_date, data_dir=SEARCH_INDEX_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea16cc7-ccfb-45a1-baca-87d3a4f00689",
   "metadata": {},
   "source": [
    "Read through the file to count lines and get an idea of timing. This took 7-8 min.\n",
    "\n",
    "Processing the full ES index can be done by extending `search_index.IndexStream`. This handles streaming through the file, skipping the `index` lines, and optionally outputting a subset of the records to a separate file (`.json.gz`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1404e372-672b-496b-81c4-002681ba8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineCount(search_index.IndexStream):\n",
    "    def _process_record(self, line, i):\n",
    "        self.n_kept += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2afde572-eb94-4a13-bc96-1270eb227641",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LineCount(SEARCH_INDEX_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b2a4695-acda-4837-be7f-a666fe3e4e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13277592it [07:52, 28071.37it/s]                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 26s, sys: 18.9 s, total: 7min 45s\n",
      "Wall time: 7min 53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b52492ef-625d-4b4b-812d-3ec0520bf9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num lines: 6,638,796\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num lines: {lc.n_kept:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a1c3d7-6e2f-405a-8835-4b26e9f91e44",
   "metadata": {},
   "source": [
    "## Pull Wikipedia records matching the blocklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a0c4b9-e61a-4fd1-8187-392ece9c2cf1",
   "metadata": {},
   "source": [
    "Having built the list of categories for which Wikipedia articles should be blocked from Dynamic Wikipedia, we can now extract the subset of such pages from the full ES index file.\n",
    "From here we get the count of the number of affected pages, and we can run further validation on their contents.\n",
    "\n",
    "First load the list of blocked categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3afbaa3a-d9b8-43a7-b1f6-e86129c5594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_df = pd.read_csv(BLOCKED_CATS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e4bc851b-44ce-401b-b0ae-533df8b60810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockIndex(search_index.IndexStream):\n",
    "    def __init__(self, data_dir, blocked_cats):\n",
    "        super().__init__(data_dir)\n",
    "        self.blocked_cats = pd.Series(blocked_cats)\n",
    "\n",
    "    def _process_record(self, line, i):\n",
    "        # Parse each index record and check if it has one of the blocked categories.\n",
    "        j = json.loads(line)\n",
    "        if self.blocked_cats.isin(j[\"category\"]).any():\n",
    "            # If so, write to the file containing the subset of blocked pages.\n",
    "            self._write_to_output(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a4fc4-9c3e-48ac-9433-2f30a90f2c4b",
   "metadata": {},
   "source": [
    "Takes ~30 min and produces a `.json.gz` file of 300-400 MB.\n",
    "\n",
    "The number of blocked pages is ~30,000, or ~0.5% of English Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7fc48be8-d89b-4ee9-a717-479d96a6c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = BlockIndex(SEARCH_INDEX_DIR, bl_df[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "01cddd1c-7e12-4578-8420-05010db75b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13277592it [28:53, 7661.51it/s]                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 51s, sys: 40.7 s, total: 28min 32s\n",
      "Wall time: 28min 53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "bi.run(output_file=BLOCKED_PAGES_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "20742aaa-0360-481f-b5bf-002e9fdc8f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num pages that will be blocked: 29,601\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num pages that will be blocked: {bi.n_kept:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "56c616aa-9687-4151-ad7c-95378ec46a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of blocked page subset: 326.0 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of blocked page subset: {BLOCKED_PAGES_INDEX.stat().st_size / (1024*1024):.1f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wikipedia]",
   "language": "python",
   "name": "conda-env-wikipedia-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
