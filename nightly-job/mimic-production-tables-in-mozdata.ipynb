{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0bae3d5-755d-4084-856b-9b9304120623",
   "metadata": {},
   "source": [
    "# Mimic Search Sanitization Tables in Staging (mozdata) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f18d61-c365-4059-b505-b11dd5df2dd4",
   "metadata": {},
   "source": [
    "### In the shared-prod environment, search terms sanitization modifies four tables:\n",
    "\n",
    "1. It adds lots of rows, one for each of a all search terms deemed sanitary from the 48 hour raw terms table, to the sanitized terms table, which we keep for 30 days\n",
    "2. It adds a row to a metadata table that records whether a run succeeded, how long it took, why it failed (if it failed), and some aggregate metrics about the terms seen in that run\n",
    "3. It adds several rows to a languages table for every language seen in the last run, and how many search terms that run saw that the job detected for each language\n",
    "4. It adds a row to a data validation table that records the proportion of all search terms in the run that displayed various characteristics (capital letters, U.S. 2010 census surnames, English language, etc)\n",
    "\n",
    "### We want to be able to run and test jobs that write to these tables without messing up the production versions. \n",
    "\n",
    "So this notebook, run start to finish, populates tables of the same schema as prod in `mozdata`. You can see the names of the mozdata versions of the tables in the `Args` attributes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ce7d40-19be-4f9d-9940-35aef9a299f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# For populating metadata table (2)\n",
    "args.sanitized_term_destination = 'mozdata.search_terms_unsanitized_analysis.prototype_sanitized_data'\n",
    "args.job_reporting_destination = 'mozdata.search_terms_unsanitized_analysis.prototype_sanitization_job_metadata'\n",
    "\n",
    "# For populating languages table (3) \n",
    "args.job_metadata_origin = 'mozdata.search_terms_unsanitized_analysis.prototype_sanitization_job_metadata'\n",
    "\n",
    "# For populating data validation table (4)\n",
    "args.job_reporting_origin = 'mozdata.search_terms_unsanitized_analysis.prototype_sanitization_job_metadata'\n",
    "args.languages_origin = 'mozdata.search_terms_unsanitized_analysis.prototype_sanitization_job_languages'\n",
    "args.data_validation_destination = 'mozdata.search_terms_unsanitized_analysis.prototype_data_validation_metrics'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73aa5ae-a685-4536-9796-6cc3c8c6758a",
   "metadata": {},
   "source": [
    "### Step 1: Populate sanitized search terms table from unsanitized search term logs and record sanitization job metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af53737-6df8-44bb-8604-7c4cac28475a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from query_sanitization import stream_search_terms, detect_pii, export_search_queries_to_bigquery, record_job_metadata\n",
    "import numpy\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "import functools\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3f9576-5934-43b7-a431-166613754d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "New row representing job run successfully added.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../assets/Names_2010Census.csv')\n",
    "census_surnames = [str(name).lower() for name in df.name]\n",
    "\n",
    "start_time = datetime.utcnow()\n",
    "\n",
    "try:\n",
    "    total_run = 0\n",
    "    total_deemed_sanitary = 0\n",
    "    summary_run_data = {}\n",
    "    summary_language_data = {}\n",
    "    \n",
    "    unsanitized_search_term_stream = stream_search_terms() # load unsanitized search terms\n",
    "    for raw_page in unsanitized_search_term_stream:\n",
    "        total_run += raw_page.shape[0]\n",
    "        pii_in_query_mask, run_data, language_data = await detect_pii(raw_page['query'], census_surnames)\n",
    "        sanitized_page = raw_page.loc[~numpy.array(pii_in_query_mask)] # ~ reverses the mask so we get the queries WITHOUT PII in them\n",
    "        total_deemed_sanitary += sanitized_page.shape[0]\n",
    "        \n",
    "        summary_language_data = dict(functools.reduce(operator.add,\n",
    "                            map(collections.Counter, [summary_language_data, language_data])))\n",
    "        summary_run_data = dict(functools.reduce(operator.add,\n",
    "                            map(collections.Counter, [summary_run_data, run_data])))\n",
    "                \n",
    "        yesterday = datetime.utcnow().date() - timedelta(days=1)\n",
    "        export_search_queries_to_bigquery(dataframe=sanitized_page, destination_table_id=args.sanitized_term_destination, date=yesterday)\n",
    "    end_time = datetime.utcnow()\n",
    "    \n",
    "    implementation_notes = \"Run with a page_size of 300k\" \n",
    "    record_job_metadata(status='SUCCESS', started_at=start_time, ended_at=end_time, destination_table=args.job_reporting_destination, total_run=total_run, total_rejected=total_run - total_deemed_sanitary, run_data=summary_run_data, language_data=summary_language_data, implementation_notes=implementation_notes)\n",
    "except Exception as e:\n",
    "    # TODO: Make this more robust in actual failure cases\n",
    "    # Maybe include the reason? Or should the logs be elsewhere for that\n",
    "    record_job_metadata(status='FAILURE', started_at=start_time, ended_at=datetime.utcnow(), destination_table=args.job_reporting_destination, failure_reason=str(e))\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64459705-d6d4-4318-8714-0c89ef63144a",
   "metadata": {},
   "source": [
    "### Step 2: Populate Languages Table (takes metadata table as input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d84e23f-f7f8-4cdd-a350-65428a30f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "456292d1-5b22-4bcb-8b54-183389d9bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "\n",
    "POPULATE_LANGUAGES_TABLE = f\"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `mozdata.search_terms_unsanitized_analysis.prototype_sanitization_job_languages`\n",
    "AS\n",
    "SELECT\n",
    "  started_at AS job_start_time,\n",
    "  lang.key AS language_code,\n",
    "  SAFE_CAST(lang.value AS int) AS search_term_count\n",
    "FROM\n",
    "  `{args.job_metadata_origin}`\n",
    "CROSS JOIN\n",
    "  UNNEST(mozfun.json.js_extract_string_map(approximate_language_proportions_json)) AS lang\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(POPULATE_LANGUAGES_TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb3ea9-09ab-4f59-aa31-dbd210a44cbf",
   "metadata": {},
   "source": [
    "### Step 3: Populate Data Validation Metrics Table (takes metadata table and languages table as input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b218d3a-ef16-4cf2-bcf6-4ea312b6ac03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_validation import calculate_data_validation_metrics, export_data_validation_metrics_to_bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffc5aca5-bb6f-43ee-b865-a971ad38edc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_i = calculate_data_validation_metrics(args.job_reporting_origin, args.languages_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a70eaf-ec64-4f32-b47b-4795268bf3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i.finished_at = df_i.finished_at.apply(lambda timestamp: timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2cf5970-f7c3-4994-b369-fac7daeae8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[]]\n"
     ]
    }
   ],
   "source": [
    "export_data_validation_metrics_to_bigquery(df_i, args.data_validation_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df1687-ef96-47c5-b196-b58c93fdad9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
