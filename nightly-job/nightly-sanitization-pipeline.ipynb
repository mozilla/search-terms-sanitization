{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0bae3d5-755d-4084-856b-9b9304120623",
   "metadata": {},
   "source": [
    "# Pipeline for Sanitizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b218d3a-ef16-4cf2-bcf6-4ea312b6ac03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from query_sanitization import stream_search_terms, detect_pii, export_search_queries_to_bigquery, export_sample_to_bigquery, record_job_metadata\n",
    "import numpy\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "import functools\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17409072-798b-41d3-8736-8c21549b5252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "args.sanitized_term_destination = 'mozdata.search_terms_unsanitized_analysis.prototype_sanitized_data'\n",
    "args.job_reporting_destination = 'mozdata.search_terms_unsanitized_analysis.prototype_sanitization_job_metadata'\n",
    "args.unsanitized_term_sample_destination = 'mozdata.search_terms_unsanitized_analysis.prototype_unsanitized_sample_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc5aca5-bb6f-43ee-b865-a971ad38edc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAFRAME COLS AT EXPORT\n",
      "Index(['timestamp', 'request_id', 'session_id', 'sequence_no', 'query',\n",
      "       'country', 'region', 'dma', 'form_factor', 'browser', 'os_family'],\n",
      "      dtype='object')\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAFRAME COLS AT EXPORT\n",
      "Index(['timestamp', 'request_id', 'session_id', 'sequence_no', 'query',\n",
      "       'country', 'region', 'dma', 'form_factor', 'browser', 'os_family'],\n",
      "      dtype='object')\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAFRAME COLS AT EXPORT\n",
      "Index(['timestamp', 'request_id', 'session_id', 'sequence_no', 'query',\n",
      "       'country', 'region', 'dma', 'form_factor', 'browser', 'os_family'],\n",
      "      dtype='object')\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAFRAME COLS AT EXPORT\n",
      "Index(['timestamp', 'request_id', 'session_id', 'sequence_no', 'query',\n",
      "       'country', 'region', 'dma', 'form_factor', 'browser', 'os_family'],\n",
      "      dtype='object')\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAFRAME COLS AT EXPORT\n",
      "Index(['timestamp', 'request_id', 'session_id', 'sequence_no', 'query',\n",
      "       'country', 'region', 'dma', 'form_factor', 'browser', 'os_family'],\n",
      "      dtype='object')\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "[[], [], []]\n",
      "New row representing job run successfully added.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Names_2010Census.csv')\n",
    "census_surnames = [str(name).lower() for name in df.name]\n",
    "\n",
    "start_time = datetime.utcnow()\n",
    "\n",
    "try:\n",
    "    total_run = 0\n",
    "    total_deemed_sanitary = 0\n",
    "    summary_run_data = {}\n",
    "    summary_language_data = {}\n",
    "    yesterday = datetime.utcnow().date() - timedelta(days=1)\n",
    "    \n",
    "    data_validation_sample = pd.DataFrame()\n",
    "    \n",
    "    unsanitized_search_term_stream = stream_search_terms() # load unsanitized search terms\n",
    "    for raw_page in unsanitized_search_term_stream:\n",
    "        total_run += raw_page.shape[0]\n",
    "        \n",
    "        one_percent_sample = raw_page.sample(frac = 0.01)\n",
    "        data_validation_sample = data_validation_sample.append(one_percent_sample)\n",
    "        \n",
    "        allow_listed_terms_page = raw_page.loc[raw_page.present_in_allow_list]\n",
    "        unsanitized_unallowlisted_terms = raw_page.loc[~raw_page.present_in_allow_list]\n",
    "\n",
    "        pii_in_query_mask, run_data, language_data = await detect_pii(unsanitized_unallowlisted_terms['query'], census_surnames)\n",
    "        sanitized_page = unsanitized_unallowlisted_terms.loc[~numpy.array(pii_in_query_mask)] # ~ reverses the mask so we get the queries WITHOUT PII in them\n",
    "        total_deemed_sanitary += sanitized_page.shape[0]\n",
    "        \n",
    "        summary_language_data = dict(functools.reduce(operator.add,\n",
    "                            map(collections.Counter, [summary_language_data, language_data])))\n",
    "        summary_run_data = dict(functools.reduce(operator.add,\n",
    "                            map(collections.Counter, [summary_run_data, run_data])))\n",
    "                \n",
    "        all_terms_to_keep = pd.concat([allow_listed_terms_page, sanitized_page])\n",
    "        all_terms_to_keep = all_terms_to_keep.drop(columns=['present_in_allow_list'])\n",
    "        \n",
    "        export_search_queries_to_bigquery(dataframe=all_terms_to_keep, destination_table_id=args.sanitized_term_destination, date=yesterday)\n",
    "    \n",
    "    end_time = datetime.utcnow()\n",
    "    \n",
    "    data_validation_sample = data_validation_sample.drop(columns=['present_in_allow_list'])\n",
    "    export_sample_to_bigquery(dataframe=data_validation_sample, sample_table_id=args.unsanitized_term_sample_destination, date=yesterday)\n",
    "    \n",
    "    implementation_notes = \"Run with a page_size of 300k\" \n",
    "    record_job_metadata(status='SUCCESS', started_at=start_time, ended_at=end_time, destination_table=args.job_reporting_destination, total_run=total_run, total_rejected=total_run - total_deemed_sanitary, run_data=summary_run_data, language_data=summary_language_data, implementation_notes=implementation_notes)\n",
    "except Exception as e:\n",
    "    # TODO: Make this more robust in actual failure cases\n",
    "    # Maybe include the reason? Or should the logs be elsewhere for that\n",
    "    record_job_metadata(status='FAILURE', started_at=start_time, ended_at=datetime.utcnow(), destination_table=args.job_reporting_destination, failure_reason=str(e))\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a70eaf-ec64-4f32-b47b-4795268bf3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
